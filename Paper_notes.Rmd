---
title: "Paper_notes"
author: "Viviana Alejandra Rodriguez Romero"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
### Watson & Crick, 1953

- Watson, J. D., and F. H. Crick. "Molecular Structure of Nucleic Acids;
a Structure for Deoxyribose Nucleic Acid." Nature 171, no. 4356 (April 25,
1953): 737-38

A new structure of the deoxyribose nucleic acid (D.N.A.). Structures proposed previously are not satisfactory. The new structure has two helical chains rotating about the same axis but in opposite directions. The chains are hydrogen bonded by pairs of single bases from each chain. Also, the pairs should be adenine with thymine, and guanine with cytosine, but it is necessary more experiments to prove this fact.     

### Tumor Analysis Best Practices Working Group, 2004

- Tumor Analysis Best Practices Working Group. "Expression Profiling-Best Practices for Data Generation and Interpretation in Clinical Trials." Nature Reviews. Genetics 5, no. 3 (March 2004): 229-37. doi:10.1038/nrg1297. https://www.ncbi.nlm.nih.gov/pubmed/14970825

Spotted cDNAs, spotted oligonucleotides and Affymetrix arrays are the most used types of microarray platforms. However, the manufacture of them varies from place to place, which decreases the comparability between results from different studies.  Then, standardization processes are required in all points of the microarray analysis for using and interpreting microarray data. Standardization ideas go from laboratory process, using the same control RNA solution, to reporting results, as the MIAME (Minimum Information About A  Microarray  Experiment) proposes.
Experimental design is the first place where the standardization is required. Here is important to consider the number of replications, tissue/cell heterogeneity, and procedural variation. Technical variability, as the quantity and quality of the RNA isolated, as the microarray controls have standardized rules.  Another step that requires standard procedures in the signal generation (normalization). Finally, robust statistical analysis that can handle multicollinearity in small samples to make good predictions are needed.

### Lipshutz  et.al., 1999

- Lipshutz, R. J., Fodor, S. P., Gingeras T. R., and Lockhart D. J.. "High Density Synthetic Oligonucleotide Arrays." Nature Genetics 21, no. 1 Suppl (January 1999): 20-24. doi:10.1038/4447. https://www.ncbi.nlm.nih.gov/pubmed/9915496 

The GeneChip probe array is a new method to design high-density, two-dimensional arrays of synthetic oligonucleotides. This array can monitor the expression levels of several genes, since oligonucleotide probes use the same method regardless of the organism under study, with good discrimination between signal and noise.

### Altman & Krzywinski, 2014 

- Altman, N., Krzywinski M. “Points of Significance: Sources of Variation.” Nature Methods 12, no. 1 (December 30, 2014): 5–6. doi:10.1038/nmeth.3224

All experiments are exposed to different sources of variability. While some sources affect the internal validity of the experiment, others affect external validity. It is essential to identify the sources of variability, to control or measure them, to have more precise and unbiased estimates. 
Biological variability, for example, phenotype and genotype data, can be controlled by fixing the levels of these variables and randomizing to treatments. 
On another hand, replication increases the precision of the estimates and allows to measure the variability from different sources. Additionally, replications in top layers have more impact on the precision than in later layers. 
Finally, blocking can control both internal and external variability, for example using siblings controls external variability and blocking by a technician controls internal variability. With this reduction in variability, blocking increases power in tests for comparisons between treatments.

### Bolstad et.al., 2003

- Bolstad BM, Irizarry RA, Astrand M, Speed TP. “A Comparison of Normalization Methods for High Density Oligonucleotide Array Data Based on Variance and Bias.” Bioinformatics (Oxford, England) 19, no. 2 (January 22, 2003): 185–93.

Normalization aims to remove non-biological variation between arrays. Three different complete-data normalization methods are proposed, all independent of the baseline array and carried at probe level. Cyclic loess uses an MA plot applied to probe intensities of two different arrays ($x_{ki}$ and $x_{kj}$), then a loess regression adjusts the probe intensities. This method can be time-consuming since, under multiple arrays, all the pairwise combinations between probes from different arrays should be done multiple times until no more normalization is required. The contrast-based method uses log-scale transformation to build the MA plots, subsequently, the intensities are adjusted using $n-1$ normalizing curves and finally, data is transformed back using exponentiation. This method is also time-consuming. Quantile normalization aims to make the intensities distributions of many arrays the same by using the mean quantile as the value of the original data.

As a comparison of performance, two baseline array methods are used, scaling and non-linear approaches. Complete-data methods showed more variance reduction as compared with baseline array methods, particularly the quantile method. Also, quantile method showed to be the best method minimizing the differences in pairwise comparisons between arrays, and less bias among the other complete-data methods. 


### Cleveland & William &  Devlin, 1988

- Cleveland, William S, Devlin S. “Locally Weighted Regression: An Approach to Regression Analysis by Local Fitting.” Journal of the American Statistical Association 83, no. 403 (1988): 596–610.

Loess stands by locally weighted regression. This is a recurrent process in which a weighted least squares estimation is used at each point using only the q nearest-neighbors. The nearest-neighbors are defined using a distance function, usually the Euclidian distance. Loess also requires a weight function which is the tricube function. The tricube function gives more weight to nearby neighbors, and the weights go down as the distance of the point of interest increases. The loess estimate is a linear combination of the observed values for the dependent variable, and it is an unbiased estimator. However, assumptions of independence, normality and constant variance of the errors should be assesed. 

Loess is more useful than regression when the relationship between independent and dependent variables is not linear. However, loess uses all the dependent variables without allowing a variable selection at the same time than building the model. Also, under a high number of independent variables, smoothing is not recommendable.


### Smyth (2004) 

- Smyth GK. "Linear Models and Empirical Bayes Methods for Assessing Differential Expression in Microarray Experiments". Statistical Applications in Genetics and Molecular Biology 3 (2004).

Lonnstedt's method to detect differential expression using a B-statistic (2002) is extended to analyze general microarray experiments. The process builds linear models for the experiment where the dependent variable is the gene expression, for example, log2(R)-log2(G); and the idea is to estimate the model coefficient that distinguishes between groups. The hierarchical model assumes a prior distribution for the coefficients and the variances for each gene. A moderated t-statistic is defined as the classic t-statistic but using the posterior variance instead of the sample variance. Under the null hypothesis, the coefficient is zero, and the moderated t-statistic follows a t-distribution where the degrees of freedom are calculated using the data. The paper also proposes the calculation of the posterior odds to measure the evidence for differential expression (B). The authors show the methodology for the estimation of the hyperparameters. Even when the gene identification using the moderate t-statistic or using B would lead similar results, B requires knowledge of more parameters as compared with moderated t. A simulation study shows that the moderated t has lower false discovery rate as compared with other methods under different scenarios.


### Tusher & Tibshirani & Chu (2001)

- Tusher VG., Tibshirani R., and Chu G. "Significance Analysis of Microarrays Applied to the Ionizing Radiation Response"." Proceedings of the National Academy of Sciences of the United States of America 98, no. 9 (April 24, 2001)

Microarrays allow the measurement of thousands of genes in only one experiment but conventional statistical methods to assess differential expression present high false discovery rate (FDR).  Significance Analysis of Microarrays (SAM) is a proposed approach to identify significant changes in gene expression while accounting for multiple comparisons. First, the proposed statistic, d(i), is a relative difference in the expresión divided by the standard desviación of the repeated measures, adding a positive constant in the denominator to adjust for small variability in gene expression. This statistic is calculated for each gene.

Then, permutations are performed, and the statistic is calculated for each permutation getting the distribution of the statistic and the expected relative difference [de(i)].  The d(i) and de(i) are compared, usually plotted together, and thresholds are specified to identify induced or repressed genes. One important thing about this approach is that the thresholds allow an asymmetric identification.  This approach has shown lower FDR as compared with fold change identification methods. 

### Goeman & Solari (2014)

- Goeman JJ, Solari A. "Multiple Hypothesis Testing in Genomics". Statistics in Medicine 33, no. 11 (May 20, 2014).

Multiple testing and correction for it are discussed. Three different perspectives for error control are described. Familywise error (FWER) that controls the probability of having at least one error in a set; the false discovery rate (FDR) that controls the expected proportion of errors among the rejections; and the false discovery proportion (FDP) that estimates the proportion of false rejections among the rejections. 

The FWER methods: 

- Bonferroni which is simple but conservative especially in cases such as in gene expression where the p-values are correlated. 

- Holm that is less conservative than Bonferroni by allowing an increasing alpha as less hypothesis remains to be tested. 

- Hochberg and Hommel assume positive dependence through stochastic ordering (PDS), which makes them less conservative as compared with Holm. 

- Westfall & Young is a sequential permutation method that rejects according to the quartiles and then recalculated the distribution until no more hypothesis are rejected. 

The FDR methods: 

- Benjamini & Hochberg

- False discovery rate control under general dependence

- False discovery rate control by resampling

- Use of false discovery rate control

The FDP methods:

- Storey’s approach and significance analysis of microarrays

- Efron’s approach

- False discovery proportion confidence

- Meinshausen’s permutation method


### Yoav & Hochberg (1995)

- Benjamini Y., and Hochberg Y. "Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing." Journal of the Royal Statistical Society. Series B (Methodological), 1995, 289–300. 

False Discovery Rate (FDR) is the expected proportion of erroneously rejected hypothesis among the rejected ones. Benjamini and Hochberg (1995) proposed an approach to multiple testing that controls the FDR. Let us assume we have $H_1, H_2, ... H_m$ hypothesis and $P_1, P_2, ..., P_m$ are their corresponding p-values. Now, the p-values are ordered from smaller to larger ($P_{(1)}, P_{(2)}, ..., P_{(m)}$). Then, let $k$ be the largest $i$ for which 
$$P_{(i)} \leq \frac{i}{m}q^*$$

then, reject all $H_{(i)}$, $i=1,2,...,k$.

The simulation study showed that the power of all methods decreases as the number of hypotheses increases. However, the power of this method is larger than the other studied methods. 


### Storey & Tibshirani (2003)

- Storey JD., and Tibshirani R. "Statistical Significance for Genomewide Studies."" Proceedings of the National Academy of Sciences of the United States of America 100, no. 16 (August 5, 2003): 9440-45. https://doi.org/10.1073/pnas.1530509100.

Storey and Tibshirani (2003) proposed an approach to measuring statistical significance controlling the false discovery rate (FDR). The q-value is a measure of the significance of each hypothesis regarding the FDR. Thus, it represents the expected proportion of false positives obtained when calling that hypothesis significant.    

First, the proportion of hypothesis that are truly null ($\pi_0$) is estimated. Then, the FDR is estimated too. Finally, the q-value for each hypothesis is calculated as:

$$\hat{q}(p_i)=min_{t \geq p_i} \frac{\hat{\pi}_0 mt}{ \# \{ p_i \leq t \}}$$ 
This methodology is less conservative than other methodologies that control the FDR. q-values can be used to identify hypothesis which should be investigated further. 


### Storey & Tibshirani (2003)

- Ackermann M., Strimmer K. "A General Modular Framework for Gene Set Enrichment Analysis". BMC Bioinformatics 10, no. 1 (2009): 47. doi:10.1186/1471-2105-10-47. 

Methods to assess whether a set of genes is enriched are discussed. Figure 1 shows a general process. First, the identification of differentiated express genes (at gene level) is made.  Second, work with some transformation of the differential expression can be more useful. Third, define the hypothesis to study. The options are the competitive null hypothesis (Q1), self-contained null hypothesis (Q2), and nested null hypothesis (Q3).  Fourth, choose the statistics to assess the hypothesis (gene set statistic). Fifth, choose the statistic approach to calculate the p-value. Q1 implies gene sampling and Q2 implies sample label permutation. 

The simulation study showed that the election of the statistic in the first step does not seem to change the results very much. In the third step, median and the Wilcoxon approaches seem to perform better, especially if the hypothesis is Q1. The use of global test does not seem to improve the results. The conclusion section gives details of the best option to choose in each step considering different issues.

![width='0.05%'](ackerman_strimmer2009.png)


### Hung et.al., 2012

- Hung JH, Yang TH, Hu Z, Weng Z, DeLisi C. "Gene Set Enrichment Analysis: Performance Evaluation and Usage Guidelines". Briefings in Bioinformatics 13, no. 3 (May 2012): 281-91. doi:10.1093/bib/bbr049. 

Key components of performing gene set enrichment analysis.

- Data preprocessing: Normalization (RMA and MAS 5.0) and imputation of missing data (K nearest neighbors, singular value decomposition, or least square regression models).
- Single gene statistics:  median and the Wilcoxon.
- Gene set-level statistics: Q1 or Q2, but Q2 keeps the relationship between genes. Use controlled mutual coverage (CMC). Wilcoxon rank sum test and WKS had the highest CMC.
- Estimating significance: get p-value according to hypothesis and statistic. Better to use simulated backgrounds than analytical backgrounds.
- Correction for multiple testing: do FDR.

